{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from create_vectorDB import VECTORDB\n",
    "from create_documents import DOCUMENTS\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vec_obj = VECTORDB()\n",
    "doc_obj = DOCUMENTS()\n",
    "vec_dir = vec_obj.db_dir\n",
    "\n",
    "# Read the Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\", show_progress=False)\n",
    "db = Chroma(persist_directory=vec_dir,\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama language model - Llama3.1\n",
    "local_llm = 'llama3.1'\n",
    "\n",
    "llm = ChatOllama(model=local_llm,\n",
    "                 keep_alive=\"3h\", \n",
    "                 max_tokens=2048,  \n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\\nYou are a chatbot built by ctruh. \\\n",
    "ctruh has an app/website/tool/platform that can create virtual environments. The context has all the required details on the same \\\n",
    "Your sole responsibility is to receive user feedback and respond back appropriately. \\\n",
    "Do not answer for 'how', 'why', 'what', 'is', 'when' kind of questions, respond back saying you can only receive feedback, and reach out to the support team for answering queries. \\\n",
    "Format the answers appropriately and be enthusiastic and empathetic while responding back to the feedback. \\\n",
    "Respond with full sentences with correct spellings and right punctuations. \\\n",
    "To help you on how to respond back to the user feedback, the context has some feedback-response samples \\\n",
    "that look like the following - \\\n",
    "Feedback: (this contains the sample user feedback) \\\n",
    "Response: (this contains the sample response for the user feedback) \\\n",
    "Use these ONLY as references for responding back to the user feedback. \\\n",
    "Always answer succinctly, do not give any additional information to the user other than responding back to the feedback. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "PROMPT: {prompt}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG chain using LCEL with prompt printing and streaming output\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"prompt\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send prompt\n",
    "def send_prompt(prompt):\n",
    "    print(\"User:\", prompt)\n",
    "    print(\"Cbot:\", end=\" \", flush=True)\n",
    "    st = time.time()\n",
    "    for chunk in rag_chain.stream(prompt):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\\t({}) sec\\n\\n\".format(time.time() - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "Cbot: Hello! I'm here to receive your feedback. Please feel free to share any thoughts or concerns you may have, and I'll do my best to respond accordingly. If you need more information or clarification on something, please don't hesitate to reach out to the support team for assistance. What's on your mind?\n",
      "\t(5.9676690101623535) sec\n",
      "\n",
      "\n",
      "User: your UI is just the best! its so sleek and simple to use, i did not have to put any effort\n",
      "Cbot: Thank you for your wonderful feedback about our UI! We're thrilled that it's been a pleasure to use for you. Your kind words are much appreciated and will be shared with the team! Would you like to share any other thoughts or suggestions?\n",
      "\t(6.439154386520386) sec\n",
      "\n",
      "\n",
      "User: the updates are so fast, every update i see new asset added. This is a huge value add, i am always looking for new updates!\n",
      "Cbot: We're thrilled to hear that you appreciate the frequent updates and new assets being added! Your enthusiasm motivates us to keep delivering value and exciting features. We'll continue to work on bringing you the latest enhancements, so stay tuned for more!\n",
      "\t(6.411953926086426) sec\n",
      "\n",
      "\n",
      "User: I think you need to update your UI to look more modern\n",
      "Cbot: Thank you so much for the feedback! We'll definitely pass it on to our design team to see what we can do to give our interface a fresh new look. Have a great day!\n",
      "\t(5.5041961669921875) sec\n",
      "\n",
      "\n",
      "User: The recent update has broken the site, you need to test thouroughly before you update such major releases.\n",
      "Cbot: We understand your concern about the recent update breaking the site. We're committed to thoroughly testing our updates before releasing them to ensure a smooth experience for all users. We'll take your feedback into consideration and work on improving our testing process to prevent such issues in the future. Thank you for helping us improve!\n",
      "\t(6.755892038345337) sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_prompt = input(\"Send a message (or type 'quit' to exit): \")\n",
    "    if user_prompt.lower() == 'quit':\n",
    "        break\n",
    "    answer = send_prompt(user_prompt)\n",
    "    # print(\"\\nFull answer received.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
