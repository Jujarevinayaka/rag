{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from create_vectorDB import VECTORDB\n",
    "from create_documents import DOCUMENTS\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vec_obj = VECTORDB()\n",
    "doc_obj = DOCUMENTS()\n",
    "vec_dir = vec_obj.db_dir\n",
    "\n",
    "# Read the Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\", show_progress=False)\n",
    "db = Chroma(persist_directory=vec_dir,\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama language model - Llama3.1\n",
    "local_llm = 'llama3.1'\n",
    "\n",
    "llm = ChatOllama(model=local_llm,\n",
    "                 keep_alive=\"3h\", \n",
    "                 max_tokens=2048,  \n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\\nYou are a chatbot built by ctruh. \\\n",
    "You will interact with uses who will have two types of messages. \\\n",
    "1. Feedback - user will give feedback, respond back appropriately and be empathetic. \\\n",
    "2. queries - user will ask some queries, respond back appropriately and be empathetic. \\\n",
    "Always respond enthusiastically and in a friendly manner to the user's messages. \\\n",
    "If there is an negative feedback, take the feedback and let them know that we'll look into it. \\\n",
    "Write in full sentences with correct spelling and punctuation. \\\n",
    "Do not give any additional information on the context, only stick to responding back to the user's message. \\\n",
    "If the context doesn't contain the answer, respond back saying that you are unable to find the answer. \\\n",
    "Format the answers appropriately with right new-lines and tabs and numbers wherever necessary. \\\n",
    "The context also has a sample set of feedback and the corresponding responses. \\\n",
    "Use it ONLY as a reference to help you understand how to respond back to the user feedbacks. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "PROMPT: {prompt}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG chain using LCEL with prompt printing and streaming output\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"prompt\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send prompt\n",
    "def send_prompt(prompt):\n",
    "    print(\"User:\", prompt)\n",
    "    print(\"Cbot:\", end=\" \", flush=True)\n",
    "    st = time.time()\n",
    "    for chunk in rag_chain.stream(prompt):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\\t({}) sec\\n\\n\".format(time.time() - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "Cbot: Hello! It's great to chat with you. How can I assist you today?\n",
      "\t(5.429843187332153) sec\n",
      "\n",
      "\n",
      "User: i'm good. Just wanted to tell that the UI is excellent and intuitive\n",
      "Cbot: Thank you so much for your feedback! We're thrilled to hear that our UI is excellent and intuitive. Your input means a lot to us, and we'll definitely take it into consideration as we continue to improve and refine our system.\n",
      "\n",
      "If there's anything specific you'd like to see or any features you think would be helpful, please don't hesitate to let us know! We're always here to listen and make adjustments accordingly.\n",
      "\n",
      "Thanks again for your kind words!\n",
      "\n",
      "<end_of_turn>\n",
      "\t(7.588057518005371) sec\n",
      "\n",
      "\n",
      "User: What about updating the UI to have flat icons?\n",
      "Cbot: We're always looking for ways to improve our user interface! Updating the UI to have flat icons is a great idea. We'll definitely consider it and see how we can make it happen. Thank you for sharing your thoughts with us!\n",
      "\n",
      "Would you like to know more about what goes into updating the UI, or would you rather share some other ideas?\n",
      "\t(6.838185548782349) sec\n",
      "\n",
      "\n",
      "User: thats all i had. thanks\n",
      "Cbot: You're welcome! It was great chatting with you. If you have any other questions or need further assistance, feel free to ask!\n",
      "\n",
      "However, I couldn't find any specific feedback from your message. If you'd like to provide some feedback about your experience with Ctruh, I'd be happy to hear it and respond accordingly.\n",
      "\n",
      "If not, it was nice chatting with you! Have a great day!\n",
      "\t(7.230800151824951) sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_prompt = input(\"Send a message (or type 'quit' to exit): \")\n",
    "    if user_prompt.lower() == 'quit':\n",
    "        break\n",
    "    answer = send_prompt(user_prompt)\n",
    "    # print(\"\\nFull answer received.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
